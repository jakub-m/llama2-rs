%% https://mermaid.live/edit#pako:eNqVVF2PmkAU_SuT--QmrFlABXlomnT71m2abZMmXRoyylUJDKPjgFrjf9_5AMR1N6Y-yNw759x7zp2BI8x5ihDBouC7-YoKSX49xiVRP2QzTNOsXA5i6NZ2i5CXLW6SAkuyJ2nG_pKB5Pmd3YyhWewTV1H3NnjRsBhs0AAE25ZcMAVTuOenn99V0ALvzpiESpnsMFuupMURlRgSm2mqf-uEFPSA4krKzGqZdeUbwKVTcn__Scvu9OtEp_I9QYZxjbChLTfTebuzSzZKxe_NpehGuk22-luFmwrFQZHM8416Vc60YFSyqlC1Fe7zeXS6s9rvAWy-jQzX1G315ahbqX8yp_MV9mSejzuvk96gHbLm27fT3iW5tplf2Wy47ztVrL6b_Jab_MKN5Srp51nXWkT9fyJqWlSoeOZ5ewr9Ih-Mou6bqm-Zqi9MWa7R0r-vgq_RnvYz__GVDEzfrqa9KeZeGlw7EBPlH7H0oXecvO0Wl-DAUmQpRFJU6ABDwagO4agxMcgVMowhUssUF7QqpH7JT4q2puUfzlnLFLxariBa0GKromqdUomPGV0KyrqswDJF8YVXpYTIG4WBqQLREfYQuZ4_HPnhw2Q0DsbudOy7Dhwg8t3hWGVH4zD03SAM3JMD_0zfh2HoBd7E84NgNPH9YDp1ANNMcvFkv3nm03d6BU5MgOc


flowchart TD

embedding("embedding <br/> [seq_len x dim] <br/> (tok)")
x_1("x <br/> [dim]")
rmsnorm_1(("RMSNorm <br/> [dim]"))
rms_att_weight("RMS att. weight <br/> [L x dim] <br/> (layer)")
xb_1("xb <br/> [dim]")

%% This is for every layer    

embedding --> x_1
x_1 -->rmsnorm_1
rms_att_weight --> rmsnorm_1
rmsnorm_1 --> xb_1

w_q("Wq <br/> [L x dim x dim] <br/> (layer)")
query("query <br/> [dim]")
w_q --> matmul_q(("@"))
xb_1 ---> matmul_q
matmul_q --> query

key("key cache <br/> [L x seq_len x kv_dim] (layer, pos)")
w_k("Wk <br/> [L x dim x kv_dim] <br/> (layer)")
w_k --> matmul_k(("@"))
xb_1 ---> matmul_k
matmul_k --> key

w_v("Wv <br/> [L x dim x kv_dim] <br/> (layer)")
value("value cache <br/> [L x seq_len x kv_dim] <br/> (layer, pos)
")
w_v --> matmul_v(("@"))
xb_1 ---> matmul_v
matmul_v --> value

rope_q(["RoPE (pos)"])
query --> rope_q

rope_k(["RoPE (pos)"])
key --> rope_k

%% This is for every attention head
rope_q --[n_heads x head_size] (head)--> att_q
rope_k --[???]--> att_k
value --[???]--> att_v
subgraph Each attention head
    att_q("query slice") --> att_dot_1
    att_k("key slice") --> att_dot_1
    att_dot_1(["dot"]) --> att
    att("attention scores") --> att_softmax
    att_softmax(["softmax"])
    att_v("value slice")
    att_softmax-->att_dot_2
    att_v-->att_dot_2
    att_dot_2(["dot"]) --> att_xb
    att_xb["xb <br/> [dim]"]
end

att_xb-->matmul_xb2(("@"))
w_o("Wo </br> [L x dim x dim] </br> (layer)")-->matmul_xb2
matmul_xb2-->xb2
xb2("xb2 <br/> [dim]")-->add_x_xb2
x_1-->add_x_xb2
x_2("x' <br/> [dim]")
add_x_xb2(("+"))-->x_2

fnn_rmsnorm(("RMSNorm <br/> [dim]"))
rms_fnn_w("Wfnn <br/> [L x dim] <br/> (layer)")-->fnn_rmsnorm
x_2-->fnn_rmsnorm
xb_2("xb' <br/> [dim]")
fnn_rmsnorm-->xb_2
xb_2-->fnn_matmul_hb
    xb_2-->fnn_matmul_hb2
subgraph FNN with SwiGLU
    fnn_matmul_hb(("@"))
    fnn_hb("hb <br/> [hidden_dim]")
    w1("w1 <br/> [L x dim x hidden_dim] <br/> (layer)")
    
    w1 --> fnn_matmul_hb
    fnn_matmul_hb-->fnn_hb

    fnn_matmul_hb2(("@"))
    fnn_hb2("hb2 <br/> [hidden_dim]")
    w3("w3 <br/> [L x dim x hidden_dim] <br/> (layer)")
    
    w3 --> fnn_matmul_hb2
    fnn_matmul_hb2-->fnn_hb2
end 